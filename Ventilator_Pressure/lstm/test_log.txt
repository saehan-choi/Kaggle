# layer 4개, learning late 1e-5
# train_loss : 5.539948789470122
# test_loss : 5.515568859756688
# epochs : 1
# train_loss : 5.026448615239719
# test_loss : 5.077263838041688
# epochs : 2
# train_loss : 4.898536382899382
# test_loss : 5.500630756103348
# epochs : 3
# train_loss : 4.660965951103232
# test_loss : 5.28152266944327
# epochs : 4
# train_loss : 4.460199014496638
# test_loss : 4.731254025677481
# epochs : 5
# train_loss : 4.374208078493838
# test_loss : 4.832449437451894
# epochs : 6
# train_loss : 4.319874487388508
# test_loss : 4.2871177729815715
# epochs : 7
# train_loss : 4.2645046310969015
# test_loss : 4.3417805292399665
# epochs : 8
# train_loss : 4.226800460960838
# test_loss : 4.9760100044634035
# epochs : 9
# train_loss : 4.199664523221769
# test_loss : 4.337560107139426
# epochs : 10
# train_loss : 4.178482550891453
# test_loss : 4.093692953124532
# epochs : 11
# train_loss : 4.160073126585495
# test_loss : 4.56752236475052
# epochs : 12
# train_loss : 4.1171210687403015
# test_loss : 3.9293933587103353
# epochs : 13
# train_loss : 4.088320764005126
# test_loss : 3.8902959824714527
# epochs : 14
# train_loss : 4.096875668550679
# test_loss : 3.8947275149796954
# epochs : 15
# train_loss : 4.049489724335797
# test_loss : 4.356393519903544
# epochs : 16
# train_loss : 4.041316307954439
# test_loss : 4.922320342740224
# epochs : 17
# train_loss : 4.020951365479416
# test_loss : 4.788554424619497
# epochs : 18
# train_loss : 4.008309847204035
# test_loss : 4.3439861945136355
# epochs : 19
# train_loss : 4.00137157041621
# test_loss : 4.867438963731931
# epochs : 20
# train_loss : 3.9770297887969273
# test_loss : 4.267434321786351
# epochs : 21
# train_loss : 3.9646678284210393
# test_loss : 4.483865387994531
# epochs : 22
# train_loss : 3.955342513370001
# test_loss : 4.684733577586181
# epochs : 23
# train_loss : 3.9458412328154946
# test_loss : 4.357413325043966
# epochs : 24
# train_loss : 3.934865171783274
# test_loss : 4.620641671848007
# epochs : 25
# train_loss : 3.9184700061199558
# test_loss : 4.539404852309858
# epochs : 26
# train_loss : 3.9131407372018683
# test_loss : 4.71608194241879
# epochs : 27
# train_loss : 3.889913989130708
# test_loss : 4.241091177096612
# epochs : 28
# train_loss : 3.8844478548426284
# test_loss : 3.670275734952816
# epochs : 29
# train_loss : 3.857867419383533
# test_loss : 4.3081628008136255
# epochs : 30
# train_loss : 3.8405991086643367
# test_loss : 4.40670523984498
# epochs : 31
# train_loss : 3.824246591240638
# test_loss : 4.476096956353646
# epochs : 32
# train_loss : 3.8132696724252573
# test_loss : 4.071948413876372
# epochs : 33
# train_loss : 3.7919665580667608
# test_loss : 4.261076730894259
# epochs : 34
# train_loss : 3.779443866929876
# test_loss : 4.390302622605862
# epochs : 35
# train_loss : 3.767673233780063
# test_loss : 4.026356944073612
# epochs : 36
# train_loss : 3.76288847685108
# test_loss : 4.054965775392847
# epochs : 37
# train_loss : 3.744921094179403
# test_loss : 3.5867144758966165

# 해당모델은 데이터가 600만개이기 때문에 더 빠르고 좋은 모델을 해야할 것 같음.
# epoch가 37임에도 불구하고 loss의 감소폭이 낮았음
# 더 큰 neural net모델로 학습시키면 될지는 모르겠음.

# layer 8개, learning late 1e-5
# epochs : 0
# train_loss : 5.102260490472803
# test_loss : 5.022280330251219
# epochs : 1
# train_loss : 4.8227223187409765
# test_loss : 4.992497871332621
# epochs : 2
# train_loss : 4.577371179616896
# test_loss : 5.00779243455143

# layer 5개, learning late 1e-4
# epochs : 0
# train_loss : 2.718128473456803
# test_loss : 2.333851405056954
# epochs : 1
# train_loss : 2.299724053919805
# test_loss : 2.191647007697966
# epochs : 2
# train_loss : 2.2322942827327816
# test_loss : 2.1582692650207025
# epochs : 3
# train_loss : 2.197776448517181
# test_loss : 2.1733523507228205
# epochs : 4
# train_loss : 2.1747583202733
# test_loss : 2.137043643288496


# layer 5개, learning late 1e-3
# epochs : 0
# train_loss : 2.5697651449160324
# test_loss : 2.232156941274128
# epochs : 1
# train_loss : 2.2550047015807677
# test_loss : 2.2087789571519116
# epochs : 2
# train_loss : 2.2055258648627913
# test_loss : 2.192555686836215
# epochs : 3
# train_loss : 2.178981720486069
# test_loss : 2.160062007471714
# epochs : 4
# train_loss : 2.163216429517969
# test_loss : 2.1035426846080214
# epochs : 5
# train_loss : 2.154037190326109
# test_loss : 2.1012010635415037
# epochs : 6
# train_loss : 2.1451386107189765
# test_loss : 2.1094529110452402
# epochs : 7
# train_loss : 2.1387270704594297
# test_loss : 2.0839542135090614
# epochs : 8
# train_loss : 2.1351937197063253
# test_loss : 2.1053806724360613
# epochs : 9
# train_loss : 2.131721286169577
# test_loss : 2.092514282651443



# layer 4개, learning late 1e-3, dropout0.2
epochs : 0
train_loss : 2.371493131026264
test_loss : 2.21524668778841
epochs : 1
train_loss : 2.187329764624902
test_loss : 2.142488212009629
epochs : 2
train_loss : 2.155077914778475
test_loss : 2.1433401774547893
epochs : 3
train_loss : 2.1388621018230163
test_loss : 2.1064473780806345
epochs : 4
train_loss : 2.129047172528112
test_loss : 2.086585478851234
epochs : 5
train_loss : 2.121705137878939
test_loss : 2.0915731888655515
epochs : 6
train_loss : 2.116918041948505
test_loss : 2.079739104225552
epochs : 7
train_loss : 2.1133708215765865
test_loss : 2.0774615484261183 
epochs : 8
train_loss : 2.109369979589092
test_loss : 2.06076041339627  
epochs : 9
train_loss : 2.1057211024157114
test_loss : 2.0551120588439393 
epochs : 10
train_loss : 2.1040023962975427
test_loss : 2.0583471691083806 
epochs : 11
train_loss : 2.103137254930759
test_loss : 2.0560180635160883
epochs : 12
train_loss : 2.1002249045730883
test_loss : 2.0590200315477487 
epochs : 13
train_loss : 2.099413536623413
test_loss : 2.0446499135515093
epochs : 14
train_loss : 2.098454596146789
test_loss : 2.0502181278654272
epochs : 15
train_loss : 2.0963306349773787
test_loss : 2.0522581035486587
epochs : 16
train_loss : 2.095265715968797
test_loss : 2.053852820952649
epochs : 17
train_loss : 2.095623514987609
test_loss : 2.0429001718143263
epochs : 18
train_loss : 2.0935955606478407
test_loss : 2.0581301923047195
epochs : 19
train_loss : 2.092420669200689
test_loss : 2.0585767729632547



# LSTM layer5 lr:1e-3 
loss : 2.964985564523935
loss : 2.5169219449579714
loss : 2.4356795304179193
loss : 2.3510321416020394
loss : 2.310487106961012
loss : 2.2975516669273377
loss : 2.3085619390070438
loss : 2.243274192893505
loss : 2.2694867755770685
loss : 2.2287792989373205
loss : 2.2039931182444095
loss : 2.189113345628977
loss : 2.2100906424045563
loss : 2.2207496501743793

epochs:0

loss : 2.1655990561068057
loss : 2.189119130605459
loss : 2.199092932111025
loss : 2.171206015241146
loss : 2.1593871790766714
loss : 2.178327993798256
loss : 2.1956354063928125
loss : 2.146607332661748
loss : 2.176121027326584
loss : 2.1551261481940744
loss : 2.1416625937461853
loss : 2.1294027793765067
loss : 2.154940817898512
loss : 2.1683172455489634

epochs:1

loss : 2.1217391797304153
loss : 2.1425878470122814
loss : 2.156635062366724
loss : 2.1319071710467337
loss : 2.1239450816690923
loss : 2.144685505074263
loss : 2.1600084067165852
loss : 2.116093130582571
loss : 2.142860083460808
loss : 2.1280589735031126
loss : 2.117565634548664
loss : 2.103365409910679
loss : 2.132170771753788
loss : 2.144339664721489

epochs:2

loss : 2.100707788181305
loss : 2.120610515612364
loss : 2.1351099329590797
loss : 2.112361108905077
loss : 2.1067592037439344
loss : 2.1284458887815476
loss : 2.142975116032362
loss : 2.0995000418543817
loss : 2.126231261789799
loss : 2.11558799661994
loss : 2.1037937438070773
loss : 2.089864967614412
loss : 2.1204922008275986
loss : 2.13064885289073

epochs:3

loss : 2.088552997213602
loss : 2.1089869039714335
loss : 2.1236500915050507
loss : 2.1017423746943473
loss : 2.094919259417057
loss : 2.115596835005283
loss : 2.129503973317146
loss : 2.087837731796503
loss : 2.115576780575514
loss : 2.1052582766890526
loss : 2.0939567933797836
loss : 2.079297139853239
loss : 2.1102277055978775
loss : 2.119914819073677

epochs:4

loss : 2.0778386358618737
loss : 2.096540800744295
loss : 2.112968589115143
loss : 2.0906572200119498
loss : 2.084615100544691
loss : 2.1079014194905756
loss : 2.119712691807747
loss : 2.0780346157371996
loss : 2.1064359517633915
loss : 2.09556345769763
loss : 2.0859616567254067
loss : 2.070677142459154
loss : 2.1022522630929945
loss : 2.112169142240286

epochs:5

loss : 2.0708406057655813
loss : 2.0863806347072122
loss : 2.104266509068012
loss : 2.0827047698676586
loss : 2.0775194142639637
loss : 2.0988670941472054
loss : 2.1103361147761346
loss : 2.0696899866998195
loss : 2.097009291934967
loss : 2.085990779328346
loss : 2.0778094493985177
loss : 2.064593465411663
loss : 2.0960704513609407
loss : 2.106050274890661

epochs:6

loss : 2.0639028587520123
loss : 2.081398839277029
loss : 2.096472635680437
loss : 2.074718997824192
loss : 2.07047648165822
loss : 2.091391950905323
loss : 2.1053167147219183
loss : 2.0634614592432974
loss : 2.093728459197283
loss : 2.082962884044647
loss : 2.0757338357269766
loss : 2.0586234563708303
loss : 2.0911328334867956
loss : 2.100422961550951

epochs:7

loss : 2.0582617859840395
loss : 2.0749101138174533
loss : 2.094287247258425
loss : 2.0721876839101316
loss : 2.0668288939237596
loss : 2.08588392534256
loss : 2.1000258917689325
loss : 2.0593963434815405
loss : 2.0875255508244037
loss : 2.0761222662866117
loss : 2.0696246585547926
loss : 2.052439455384016
loss : 2.0867333554327487
loss : 2.0956146385669707

epochs:8

loss : 2.0553183348059654
loss : 2.070233976829052
loss : 2.088570888990164
loss : 2.0659997363090516
loss : 2.061789189529419
loss : 2.0825208921313285
loss : 2.0950064753413202
loss : 2.055650197249651
loss : 2.0824532759964467
loss : 2.0712116546332835
loss : 2.065281249862909
loss : 2.048255490642786
loss : 2.0796405856907367
loss : 2.0894278608739376

epochs:9

loss : 2.048339263910055
loss : 2.063444316661358
loss : 2.0831027735352516
loss : 2.0616000377058983
loss : 2.0569323861837385
loss : 2.0765275436997412
loss : 2.088696135020256
loss : 2.0497771980285644
loss : 2.076599765986204
loss : 2.0655353109538557
loss : 2.0587377978146075
loss : 2.042604998391867
loss : 2.074936325788498
loss : 2.0857387589752676

epochs:10

loss : 2.044317096942663
loss : 2.061098129117489
loss : 2.079274477982521
loss : 2.057668638265133
loss : 2.0528151031792166
loss : 2.072608211791515
loss : 2.0837976138591765
loss : 2.0488634506464005
loss : 2.075999624246359
loss : 2.064590003156662
loss : 2.057662161809206
loss : 2.0407900338828564
loss : 2.073086014813185
loss : 2.082442402988672

epochs:11

loss : 2.0418663337528704
loss : 2.057372353667021
loss : 2.0731688685417176
loss : 2.0547506071448325
loss : 2.049929812514782
loss : 2.0685168645441534
loss : 2.0808944982349873
loss : 2.0432386215925216
loss : 2.068989635634422
loss : 2.0593762803435327
loss : 2.054312648677826
loss : 2.0374856167554856
loss : 2.0693821999132633
loss : 2.0790584107756613

epochs:12

loss : 2.0409893381118773
loss : 2.0533380843639373
loss : 2.070059120619297
loss : 2.0505316568374634
loss : 2.047434194934368
loss : 2.0653233880877493
loss : 2.077241860216856
loss : 2.0403840505898
loss : 2.0664075486660005
loss : 2.0562660551249983
loss : 2.051340631878376
loss : 2.0334925399422645
loss : 2.06781410241127
loss : 2.076645954364538

epochs:13

loss : 2.0368410918831827
loss : 2.051254676645994
loss : 2.0698766952455045
loss : 2.04850005556941
loss : 2.0429352655529978
loss : 2.0641671887815
loss : 2.0756349405229093
loss : 2.0370647991061213
loss : 2.065024120950699
loss : 2.0542725637674333
loss : 2.0490259877741335
loss : 2.034647859221697
loss : 2.0637571752667427
loss : 2.074570426738262

epochs:14

loss : 2.034632114267349
loss : 2.0488929502248765
loss : 2.066567919307947
loss : 2.0463031650066377
loss : 2.042948053562641
loss : 2.061271598947048
loss : 2.0711616837322713
loss : 2.036563323324919
loss : 2.0625195126414297
loss : 2.0521909716367723
loss : 2.0476272025167943
loss : 2.0300194669902325
loss : 2.0625786469519136
loss : 2.072935246795416

epochs:15

loss : 2.032144563025236
loss : 2.0470297846078873
loss : 2.0651802300333975
loss : 2.0426851035892963
loss : 2.0403412098884584
loss : 2.0580290529489518
loss : 2.067378635931015
loss : 2.0375438627064226
loss : 2.0620257493674754
loss : 2.048042137682438
loss : 2.045214119130373
loss : 2.030189025747776
loss : 2.059714887213707
loss : 2.0707389969825742

epochs:16

loss : 2.032905421876907
loss : 2.04550387147069
loss : 2.061765096962452
loss : 2.0420345458745954
loss : 2.0389505462825297
loss : 2.055844370085001
loss : 2.066674316138029
loss : 2.0314705193281175
loss : 2.059356884652376
loss : 2.04778057269454
loss : 2.0409372106432913
loss : 2.0273689246058466
loss : 2.0597585831701757
loss : 2.0674657297730445

epochs:17

loss : 2.0313046459317206
loss : 2.042766056007147
loss : 2.061088707780838
loss : 2.0394690725266935
loss : 2.0370755603313446
loss : 2.0556324584662913
loss : 2.065163603276014
loss : 2.030110282921791
loss : 2.0574857379078866
loss : 2.0459637254834173
loss : 2.0407505803585053
loss : 2.0246003798365595
loss : 2.055894702541828
loss : 2.066699773430824

epochs:18

loss : 2.0304749377310274
loss : 2.0424038749992848
loss : 2.0598111866652964
loss : 2.040574407851696
loss : 2.0383814629912376
loss : 2.0556456247091295
loss : 2.066367650806904
loss : 2.031960161924362
loss : 2.057459580987692
loss : 2.045670934444666
loss : 2.0408715256094934
loss : 2.0241762874901297
loss : 2.056405983924866
loss : 2.0658854102790354

epochs:19


bidirectional_LSTM
loss : 2.974686551707983
loss : 2.4345419274687767
loss : 2.3388155789792537
loss : 2.2541689353883267
loss : 2.21956767154932
loss : 2.2133522413015365
loss : 2.2227063423335554
loss : 2.161547090083361
loss : 2.181909179842472
loss : 2.1527577984392643
loss : 2.1352532895565033
loss : 2.122490229362249
loss : 2.1418196477472784
loss : 2.1492928505003452

epochs:0
loss : 2.0961119446992873
loss : 2.115859959977865
loss : 2.1275839485764503
loss : 2.1008301833570004
loss : 2.0932158231258393
loss : 2.1090790391623973
loss : 2.1223957437098027
loss : 2.083396762150526
loss : 2.1082363460481166
loss : 2.0915077045440675
loss : 2.0758146157324315
loss : 2.0631575625360012
loss : 2.0892510415256025
loss : 2.1003618839085103

epochs:1
loss : 2.05515018234849
loss : 2.0735778336584567
loss : 2.0859041362524033
loss : 2.065742891317606
loss : 2.0620139781832694
loss : 2.0772318618535994
loss : 2.0893474039793016
loss : 2.051649869889021
loss : 2.078229294496775
loss : 2.0670182136237623
loss : 2.0528809297382833
loss : 2.0404101883530616
loss : 2.0681366960048675
loss : 2.0802587644398214

epochs:2
loss : 2.0371121675431727
loss : 2.052792141264677
loss : 2.069096887218952
loss : 2.050138093173504
loss : 2.045754915237427
loss : 2.0623295502722265
loss : 2.0735933118522167
loss : 2.03742603828907
loss : 2.063667188721895
loss : 2.0527351428747176
loss : 2.0399018319308757
loss : 2.025697171843052
loss : 2.056734972435236
loss : 2.0672202945828437

epochs:3
loss : 2.0263411787509917
loss : 2.0415215185523032
loss : 2.0591987024605274
loss : 2.037423924446106
loss : 2.0347012203156947
loss : 2.0529676324665544
loss : 2.0629177540719508
loss : 2.0256506131887435
loss : 2.052998115384579
loss : 2.0414671888053415
loss : 2.0312026768922804
loss : 2.017290143030882
loss : 2.047106979203224
loss : 2.0569210611224173

epochs:4
loss : 2.016673379278183
loss : 2.0323643612325193
loss : 2.0501492117881774
loss : 2.0289330739557743
loss : 2.025895337677002
loss : 2.0431945600926875
loss : 2.0522524079859257
loss : 2.0163463685333727
loss : 2.0428054022252558
loss : 2.031485511815548
loss : 2.022864867323637
loss : 2.0084505465090277
loss : 2.0377405217051505
loss : 2.0482697568178176

epochs:5
loss : 2.008251783233881
loss : 2.024122632676363
loss : 2.0407680112123487
loss : 2.0214554369091986
loss : 2.0177693213045598
loss : 2.03565818528533
loss : 2.04330895704031
loss : 2.008471992701292
loss : 2.035236622488499
loss : 2.028400551056862
loss : 2.017824059420824
loss : 2.0024210067749024
loss : 2.032253101748228
loss : 2.0427077845811845

epochs:6
loss : 2.002130339938402
loss : 2.016344551706314
loss : 2.0340710704743863
loss : 2.0149233739614485
loss : 2.012295778012276
loss : 2.029152184098959
loss : 2.0378928206980227
loss : 2.003081335031986
loss : 2.028851024377346
loss : 2.0208776035130023
loss : 2.0135839301764964
loss : 1.999555845272541
loss : 2.028361186003685
loss : 2.0385926356434823

epochs:7
loss : 2.000373823463917
loss : 2.0126958959877492
loss : 2.0318161319851877
loss : 2.0124052389144897
loss : 2.009827557063103
loss : 2.0270519643306733
loss : 2.0325503486573697
loss : 2.000404237091541
loss : 2.025968071979284
loss : 2.015522178095579
loss : 2.009208034205437
loss : 1.9941991605520248
loss : 2.0239494237065316
loss : 2.0343836282372476

epochs:8
loss : 1.996502783715725
loss : 2.0146623771369456
loss : 2.032475097900629
loss : 2.0123940416932107
loss : 2.008317749720812
loss : 2.0248339327931406
loss : 2.032596189045906
loss : 2.0005129732847213
loss : 2.0268129056334496
loss : 2.0142986200213433
loss : 2.00717494931221
loss : 1.993240118587017
loss : 2.02201884932518
loss : 2.0328054656744

epochs:9
loss : 1.9922785872220994
loss : 2.0048896592497827
loss : 2.025796643280983
loss : 2.0068229271411897
loss : 2.0021075192332267
loss : 2.020565753817558
loss : 2.027582132536173
loss : 1.993381507062912
loss : 2.0217218921720983
loss : 2.009005812138319
loss : 2.0034266420662403
loss : 1.9881347634613513
loss : 2.018595633095503
loss : 2.028354500579834

epochs:10
loss : 1.988983940130472
loss : 2.000902442920208
loss : 2.021940546029806
loss : 2.003356352651119
loss : 2.0008845634818075
loss : 2.0161746705651282
loss : 2.0238749086141588
loss : 1.9913862607121469
loss : 2.0178020304918287
loss : 2.0074216916799545
loss : 2.0024562686800955
loss : 1.9856123050808907
loss : 2.0168210252165792
loss : 2.025608928835392

epochs:11
loss : 1.9871761906385421
loss : 1.998233645761013
loss : 2.0202722217381
loss : 2.0016650443255903
loss : 1.99723949457407
loss : 2.014253260332346
loss : 2.022191857248545
loss : 1.9885347402632236
loss : 2.0142386403262615
loss : 2.0039121952474117
loss : 1.9993464065849782
loss : 1.9831667753040791
loss : 2.0137618529260157
loss : 2.0230588984370232

epochs:12
loss : 1.9836619743853807
loss : 1.9963191931784152
loss : 2.018785225760937
loss : 1.9997778234899044
loss : 1.9968588704407215
loss : 2.0099546493053437
loss : 2.018881128436327
loss : 1.9861043792903423
loss : 2.0126957642555237
loss : 2.0016114326417447
loss : 1.998930393344164
loss : 1.9818859022796154
loss : 2.012092839175463
loss : 2.0224034145891667

epochs:13
loss : 1.9821232313454151
loss : 1.9948497252583504
loss : 2.016553327512741
loss : 1.9970249805569649
loss : 1.9946336564719678
loss : 2.010564932948351
loss : 2.017724377334118
loss : 1.9839842329204083
loss : 2.013126637661457
loss : 2.0005340097010134
loss : 1.9959420622706414
loss : 1.9796537307977677
loss : 2.0102103097856046
loss : 2.0187374882876874

epochs:14
loss : 1.979482981622219
loss : 1.9945998474776745
loss : 2.015555096018314
loss : 1.9948585409045219
loss : 1.9917201648294927
loss : 2.009798239028454
loss : 2.0148741953372955
loss : 1.9817332992196084
loss : 2.0108065309345724
loss : 2.000828210055828
loss : 1.9946592217803
loss : 1.9772500664293766
loss : 2.0084565276920796
loss : 2.0185748008787634

epochs:15
loss : 1.9787793067753314
loss : 1.9915418808877468
loss : 2.0149363771259785
loss : 1.9933644564926625
loss : 1.990351112818718
loss : 2.0047717856764793
loss : 2.0132062614262103
loss : 1.9801979513049126
loss : 2.0087021843671797
loss : 1.9966849461317062
loss : 1.9932720696568489
loss : 1.9762491060197354
loss : 2.0078577929377555
loss : 2.015605996590853

epochs:16
loss : 1.97804953866601
loss : 1.9907267387092114
loss : 2.0129990533173086
loss : 1.9932059493243695
loss : 1.9889522666811943
loss : 2.004310610252619
loss : 2.0112719653606415
loss : 1.9796155865609646
loss : 2.0067658793151377
loss : 1.995759162312746
loss : 1.9918934505999089
loss : 1.9745058312058448
loss : 2.0052031004071234
loss : 2.015640101569891

epochs:17
loss : 1.9762623134464026
loss : 1.9902080142974854
loss : 2.0093611595273018
loss : 1.9904741697788237
loss : 1.9867201932251453
loss : 2.0032242847502233
loss : 2.0108168012440206
loss : 1.9793717175543308
loss : 2.0066287994384764
loss : 1.9959149626970292
loss : 1.9915852807879448
loss : 1.9737356928288936
loss : 2.004875390267372
loss : 2.012795617312193

epochs:18
loss : 1.9773779856801033
loss : 1.9874000427424907
loss : 2.012393319118023
loss : 1.9943267399907112
loss : 1.9892683038890362
loss : 2.002770045757294
loss : 2.010282848840952
loss : 1.9789138989210129
loss : 2.007844041490555



u_out을 나누니 (잘됨 모든파라미터는 그대로)
u_out == 1 일때 테스트결과

loss : 1.123778195247054
loss : 0.9473994555413723
loss : 0.9207850250691175
loss : 0.9185076424717903
loss : 0.9114155159533024
loss : 0.8975580923646689
loss : 0.8765148522883653
loss : 0.8762707050949335

epochs:0

학습을 시작합니다.
loss : 0.87983206769526
loss : 0.8748591845244169
loss : 0.8695917403221131
loss : 0.8759251181155443
loss : 0.8758277097463608
loss : 0.8708583469718695
loss : 0.8547872777253389
loss : 0.8548743326485158

epochs:1

학습을 시작합니다.
loss : 0.8646004501223564
loss : 0.8592188221812248
loss : 0.8575892543792725
loss : 0.8641405373811721
loss : 0.8632287076145411
loss : 0.8602536339074374
loss : 0.8444660222351551
loss : 0.8453593464106322

epochs:2

학습을 시작합니다.
loss : 0.8565799734890461
loss : 0.8512126423418522
loss : 0.8512465173125267
loss : 0.8565326717376709
loss : 0.8561724138200283
loss : 0.8547434338688851
loss : 0.8395901447594166
loss : 0.840993471556902

epochs:3

학습을 시작합니다.
loss : 0.852089368262887
loss : 0.846616393712163
loss : 0.8471699339598417
loss : 0.8517349688112735
loss : 0.8517762096226216
loss : 0.8499851571589708
loss : 0.8351224452227354
loss : 0.8383051098704338

epochs:4

학습을 시작합니다.
loss : 0.8491355653643609
loss : 0.8426447906792164
loss : 0.8431771431624889
loss : 0.8488570416808129
loss : 0.8492031285732985
loss : 0.8477124571144581
loss : 0.8322270577251911
loss : 0.8347270930826663

epochs:5

학습을 시작합니다.
loss : 0.8458942143172026
loss : 0.8396847780436277
loss : 0.841027510380745
loss : 0.8453269197642803
loss : 0.8461622949987649
loss : 0.8453509345531464
loss : 0.8302098008692265
loss : 0.8331080722600221

epochs:6

학습을 시작합니다.
loss : 0.8441547975033522
loss : 0.8376170139938592
loss : 0.8395101098746062
loss : 0.8438282158344984
loss : 0.844104651877284
loss : 0.8430630259305238
loss : 0.8282985329002142
loss : 0.8310001987725496

epochs:7

학습을 시작합니다.
loss : 0.8417948169887066
loss : 0.8354206347197294
loss : 0.8372646498560905
loss : 0.8419953575462102
loss : 0.8418736234128475
loss : 0.8413004429370164
loss : 0.8269878534823656
loss : 0.8308934882700443

epochs:8

학습을 시작합니다.
loss : 0.8413606304287911
loss : 0.8347174875676632
loss : 0.8364395872056484
loss : 0.840622313991189
loss : 0.841302788323164
loss : 0.8395453922986984
loss : 0.8253588350057602
loss : 0.8286330952495337

epochs:9

학습을 시작합니다.
loss : 0.8393011035263538
loss : 0.8327049474924803
loss : 0.8342274420708418
loss : 0.8392285717517137
loss : 0.8392177407681942
loss : 0.8382582208693028
loss : 0.8231307129412889
loss : 0.8276041715562343

epochs:10

학습을 시작합니다.
loss : 0.8375255889028311
loss : 0.8309224299550056
loss : 0.8334023636043072
loss : 0.8384426416784525
loss : 0.8377419536203146
loss : 0.8368958675771951


a_lot_of_feature.pt
---------------------------------------------------
train_loss : 1.3012898341225618
test_loss : 0.969188646900281
epochs : 1
train_loss : 0.9369184537895247
test_loss : 0.8303657399524055
epochs : 2
train_loss : 0.8672956204999213
test_loss : 0.7546631387520822
epochs : 3
train_loss : 0.8329782088463871
test_loss : 0.7737616953970361
epochs : 4
train_loss : 0.8115544372326716
test_loss : 0.7476334538990247
epochs : 5
train_loss : 0.7974225984696353
test_loss : 0.7398508751798473
epochs : 6
train_loss : 0.7861231869328014
test_loss : 0.7356202452229227
epochs : 7
train_loss : 0.7778844447872457
test_loss : 0.726263092998672
epochs : 8
train_loss : 0.7717479610358046
test_loss : 0.7491103191746444
epochs : 9
train_loss : 0.767910225441852
test_loss : 0.7339431117639372
epochs : 10
train_loss : 0.7603334107638663
test_loss : 0.7243886887869172
epochs : 11
train_loss : 0.7556158895265251
test_loss : 0.6877746971705306
epochs : 12
train_loss : 0.7535177221688075
test_loss : 0.712432867226172
epochs : 13
train_loss : 0.7493823722702581
test_loss : 0.6963835348704018
epochs : 14
train_loss : 0.7464180153149585
test_loss : 0.6979369665096067
epochs : 15
train_loss : 0.7439189240702726
test_loss : 0.7028577954351333
epochs : 16
train_loss : 0.7408866523102433
test_loss : 0.7091895752682223
epochs : 17
train_loss : 0.743814958474139
test_loss : 0.691228155153364
epochs : 18
train_loss : 0.7428776719689466
test_loss : 0.6990464938170458
epochs : 19
train_loss : 0.7386151617061958
test_loss : 0.6938363873633859

we make the score 0.69!!!!!!!!!!!!!!!!!
